{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_cleaning_Tourism_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "dKSzk5768LqM",
        "outputId": "efd939a6-430a-4e1d-ed68-da2c7a242f36"
      },
      "source": [
        "import pandas as pd\n",
        "# Dataset will be stored in a Pandas Dataframe\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/DataScience-in-Tourism/Introduction_to_NLP/main/travelsomeday.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>guid</th>\n",
              "      <th>link</th>\n",
              "      <th>pubdate</th>\n",
              "      <th>author</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>source</th>\n",
              "      <th>user_id</th>\n",
              "      <th>profile_image_url</th>\n",
              "      <th>user_statuses_count</th>\n",
              "      <th>user_friends_count</th>\n",
              "      <th>user_followers_count</th>\n",
              "      <th>user_created_at</th>\n",
              "      <th>user_bio</th>\n",
              "      <th>user_location</th>\n",
              "      <th>lang</th>\n",
              "      <th>coords</th>\n",
              "      <th>Unnamed: 18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <th>https://twitter.com/_LoveCaribbean/statuses/1252926931634458624</th>\n",
              "      <th>https://twitter.com/_LoveCaribbean/statuses/1252926931634458624</th>\n",
              "      <th>2020-04-22 07:47:13</th>\n",
              "      <th>_LoveCaribbean</th>\n",
              "      <th>RT @grenadaexplorer: A tropical treasure to cherish forever ðŸŒ… For exclusive #TravelTips and island insights visit https://t.co/wFawu512ad #â€¦</th>\n",
              "      <th>A tropical treasure to cherish forever ðŸŒ… For exclusive #TravelTips and island insights visit https://t.co/wFawu512ad #Grenada #TimeToLime #PureGrenada #SpiceIsle #ILoveGrenada #DreamTodayTravelTomorrow #TravelSomeDay #WeBelieveInTravel #GrenadaDreaming #CaribbeanDreaming https://t.co/64Kg2mPH3l</th>\n",
              "      <th>Twitter Web App</th>\n",
              "      <th>1356684936</th>\n",
              "      <th>https://pbs.twimg.com/profile_images/474150473876770817/aNbE9lsf_normal.jpeg</th>\n",
              "      <th>8047</th>\n",
              "      <th>1099</th>\n",
              "      <th>3019</th>\n",
              "      <th>2013-04-16 07:55:23</th>\n",
              "      <th>#CaribbeanDreaming The diversity of the Caribbean makes it a wonderful holiday destination whatever your tastes or interests.</th>\n",
              "      <th>London</th>\n",
              "      <th>en</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                        id  ...  Unnamed: 18\n",
              "1 https://twitter.com/_LoveCaribbean/statuses/125... https://twitter.com/_LoveCaribbean/statuses/125... 2020-04-22 07:47:13 _LoveCaribbean RT @grenadaexplorer: A tropical treasure to che... A tropical treasure to cherish forever ðŸŒ… For ex... Twitter Web App 1356684936 https://pbs.twimg.com/profile_images/4741504738... 8047 1099 3019 2013-04-16 07:55:23 #CaribbeanDreaming The diversity of the Caribbe... London en NaN NaN NaN  ...          NaN\n",
              "\n",
              "[1 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoPuMeI79zwl",
        "outputId": "41364f97-10b1-4ac4-b5d4-ccece079c653"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "MultiIndex: 6027 entries, (1, 'https://twitter.com/_LoveCaribbean/statuses/1252926931634458624', 'https://twitter.com/_LoveCaribbean/statuses/1252926931634458624', '2020-04-22 07:47:13', '_LoveCaribbean', 'RT @grenadaexplorer: A tropical treasure to cherish forever ðŸŒ… For exclusive #TravelTips and island insights visit https://t.co/wFawu512ad #â€¦', 'A tropical treasure to cherish forever ðŸŒ… For exclusive #TravelTips and island insights visit https://t.co/wFawu512ad #Grenada #TimeToLime #PureGrenada #SpiceIsle #ILoveGrenada #DreamTodayTravelTomorrow #TravelSomeDay #WeBelieveInTravel #GrenadaDreaming #CaribbeanDreaming https://t.co/64Kg2mPH3l', 'Twitter Web App', 1356684936, 'https://pbs.twimg.com/profile_images/474150473876770817/aNbE9lsf_normal.jpeg', 8047, 1099, 3019, '2013-04-16 07:55:23', '#CaribbeanDreaming The diversity of the Caribbean makes it a wonderful holiday destination whatever your tastes or interests.', 'London', 'en', nan, nan) to (6027, 'https://twitter.com/clairesturz/statuses/1275265122479542273', 'https://twitter.com/clairesturz/statuses/1275265122479542273', '2020-06-22 23:11:13', 'clairesturz', 'How to Find the Best Flight Deals for Black Friday (&amp; All Year!) #travelsomeday #armchairtravel #plannowtravellater https://t.co/aO9LwQ2Yny', 'How to Find the Best Flight Deals for Black Friday (&amp; All Year!) #travelsomeday #armchairtravel #plannowtravellater https://t.co/aO9LwQ2Yny', 'Revive Social App', 874023301, 'https://pbs.twimg.com/profile_images/613107654643351552/43F_WT1I_normal.jpg', 42181, 19532, 24749, '2012-10-11 13:32:59', 'Helping you plan your best future travel adventure by sharing my tried & tested budget travel tips.  #travelsomeday #armchairtravel #dreamtodaytravellater', 'England, United Kingdom', 'en', nan, nan)\n",
            "Data columns (total 19 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   id                    0 non-null      float64\n",
            " 1   guid                  0 non-null      float64\n",
            " 2   link                  0 non-null      float64\n",
            " 3   pubdate               0 non-null      float64\n",
            " 4   author                0 non-null      float64\n",
            " 5   title                 0 non-null      float64\n",
            " 6   description           0 non-null      float64\n",
            " 7   source                0 non-null      float64\n",
            " 8   user_id               0 non-null      float64\n",
            " 9   profile_image_url     0 non-null      float64\n",
            " 10  user_statuses_count   0 non-null      float64\n",
            " 11  user_friends_count    0 non-null      float64\n",
            " 12  user_followers_count  0 non-null      float64\n",
            " 13  user_created_at       0 non-null      float64\n",
            " 14  user_bio              0 non-null      float64\n",
            " 15  user_location         0 non-null      float64\n",
            " 16  lang                  0 non-null      float64\n",
            " 17  coords                0 non-null      float64\n",
            " 18  Unnamed: 18           0 non-null      float64\n",
            "dtypes: float64(19)\n",
            "memory usage: 2.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZZ5HVhJD3tt"
      },
      "source": [
        "As we can see from here, the text data is at the index. We need to convert indexes to columns.\n",
        "\n",
        "In order to do this, first we need to give a name to columns in the index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULFnVqN4ApAP",
        "outputId": "9524e506-8971-4fb4-fd66-7786493dbb6d"
      },
      "source": [
        "df= df.rename_axis(['id1','url','url2','date','username', 'quote', 'tweet','as1','as2','as3','as4','as5','as6','as7','as8','as9','as10','as11','a12'])\n",
        "df.columns.to_list()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id',\n",
              " 'guid',\n",
              " 'link',\n",
              " 'pubdate',\n",
              " 'author',\n",
              " 'title',\n",
              " 'description',\n",
              " 'source',\n",
              " 'user_id',\n",
              " 'profile_image_url',\n",
              " 'user_statuses_count',\n",
              " 'user_friends_count',\n",
              " 'user_followers_count',\n",
              " 'user_created_at',\n",
              " 'user_bio',\n",
              " 'user_location',\n",
              " 'lang',\n",
              " 'coords',\n",
              " 'Unnamed: 18']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "qK7SY-67AqGp",
        "outputId": "bb595d58-ca94-499a-e84b-4d4722e03a8e"
      },
      "source": [
        "df.sample(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>guid</th>\n",
              "      <th>link</th>\n",
              "      <th>pubdate</th>\n",
              "      <th>author</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>source</th>\n",
              "      <th>user_id</th>\n",
              "      <th>profile_image_url</th>\n",
              "      <th>user_statuses_count</th>\n",
              "      <th>user_friends_count</th>\n",
              "      <th>user_followers_count</th>\n",
              "      <th>user_created_at</th>\n",
              "      <th>user_bio</th>\n",
              "      <th>user_location</th>\n",
              "      <th>lang</th>\n",
              "      <th>coords</th>\n",
              "      <th>Unnamed: 18</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id1</th>\n",
              "      <th>url</th>\n",
              "      <th>url2</th>\n",
              "      <th>date</th>\n",
              "      <th>username</th>\n",
              "      <th>quote</th>\n",
              "      <th>tweet</th>\n",
              "      <th>as1</th>\n",
              "      <th>as2</th>\n",
              "      <th>as3</th>\n",
              "      <th>as4</th>\n",
              "      <th>as5</th>\n",
              "      <th>as6</th>\n",
              "      <th>as7</th>\n",
              "      <th>as8</th>\n",
              "      <th>as9</th>\n",
              "      <th>as10</th>\n",
              "      <th>as11</th>\n",
              "      <th>a12</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5788</th>\n",
              "      <th>https://twitter.com/Edinburgh_forum/statuses/1273907251233935371</th>\n",
              "      <th>https://twitter.com/Edinburgh_forum/statuses/1273907251233935371</th>\n",
              "      <th>2020-06-19 05:15:31</th>\n",
              "      <th>Edinburgh_forum</th>\n",
              "      <th>RT @clairesturz: An #Edinburgh Food Tour with Eat Walk Edinburgh #travelsomeday #armchairtravel #plannowtravellater https://t.co/kyvqQVpBge</th>\n",
              "      <th>An #Edinburgh Food Tour with Eat Walk Edinburgh #travelsomeday #armchairtravel #plannowtravellater https://t.co/kyvqQVpBge</th>\n",
              "      <th>The Edinburgh Forum app</th>\n",
              "      <th>1037405206529671168</th>\n",
              "      <th>https://pbs.twimg.com/profile_images/1037407794100666368/UmIkCuEb_normal.jpg</th>\n",
              "      <th>63352</th>\n",
              "      <th>49</th>\n",
              "      <th>2058</th>\n",
              "      <th>2018-09-05 14:20:50</th>\n",
              "      <th>WE\\'RE FREE!! All things Edinburgh up for discussion, talk, sharing and retweeting!</th>\n",
              "      <th>Edinburgh, Scotland</th>\n",
              "      <th>en</th>\n",
              "      <th>NaN</th>\n",
              "      <th>NaN</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                             id  ...  Unnamed: 18\n",
              "id1  url                                                url2                                               date                username        quote                                              tweet                                              as1                     as2                 as3                                                as4   as5 as6  as7                 as8                                                as9                 as10 as11 a12      ...             \n",
              "5788 https://twitter.com/Edinburgh_forum/statuses/12... https://twitter.com/Edinburgh_forum/statuses/12... 2020-06-19 05:15:31 Edinburgh_forum RT @clairesturz: An #Edinburgh Food Tour with E... An #Edinburgh Food Tour with Eat Walk Edinburgh... The Edinburgh Forum app 1037405206529671168 https://pbs.twimg.com/profile_images/1037407794... 63352 49  2058 2018-09-05 14:20:50 WE\\'RE FREE!! All things Edinburgh up for discu... Edinburgh, Scotland en   NaN  NaN NaN  ...          NaN\n",
              "\n",
              "[1 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "C_12EjF4Ddwi",
        "outputId": "1f9895ab-b1cc-427a-a3c3-d347a1d91152"
      },
      "source": [
        "df.reset_index(inplace=True)\n",
        "df.sample(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id1</th>\n",
              "      <th>url</th>\n",
              "      <th>url2</th>\n",
              "      <th>date</th>\n",
              "      <th>username</th>\n",
              "      <th>quote</th>\n",
              "      <th>tweet</th>\n",
              "      <th>as1</th>\n",
              "      <th>as2</th>\n",
              "      <th>as3</th>\n",
              "      <th>as4</th>\n",
              "      <th>as5</th>\n",
              "      <th>as6</th>\n",
              "      <th>as7</th>\n",
              "      <th>as8</th>\n",
              "      <th>as9</th>\n",
              "      <th>as10</th>\n",
              "      <th>as11</th>\n",
              "      <th>a12</th>\n",
              "      <th>id</th>\n",
              "      <th>guid</th>\n",
              "      <th>link</th>\n",
              "      <th>pubdate</th>\n",
              "      <th>author</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>source</th>\n",
              "      <th>user_id</th>\n",
              "      <th>profile_image_url</th>\n",
              "      <th>user_statuses_count</th>\n",
              "      <th>user_friends_count</th>\n",
              "      <th>user_followers_count</th>\n",
              "      <th>user_created_at</th>\n",
              "      <th>user_bio</th>\n",
              "      <th>user_location</th>\n",
              "      <th>lang</th>\n",
              "      <th>coords</th>\n",
              "      <th>Unnamed: 18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3643</th>\n",
              "      <td>3644</td>\n",
              "      <td>https://twitter.com/clairesturz/statuses/12627...</td>\n",
              "      <td>https://twitter.com/clairesturz/statuses/12627...</td>\n",
              "      <td>2020-05-19 12:33:25</td>\n",
              "      <td>clairesturz</td>\n",
              "      <td>Learning to Surf in #Fuerteventura with Planet...</td>\n",
              "      <td>Learning to Surf in #Fuerteventura with Planet...</td>\n",
              "      <td>Revive Social App</td>\n",
              "      <td>874023301</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/613107654...</td>\n",
              "      <td>41049</td>\n",
              "      <td>19534</td>\n",
              "      <td>24791</td>\n",
              "      <td>2012-10-11 13:32:59</td>\n",
              "      <td>Helping you plan your best future travel adven...</td>\n",
              "      <td>England, United Kingdom</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id1  ... Unnamed: 18\n",
              "3643  3644  ...         NaN\n",
              "\n",
              "[1 rows x 38 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6tt6HPLDsSH",
        "outputId": "356e1226-e6ff-4d49-d092-17faae8685fd"
      },
      "source": [
        "df.tweet.sample(5).to_list()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Riviera Guesthouse in Whitby â€“ My Yorkshire Foodie Adventure #travelsomeday #armchairtravel #plannowtravellater https://t.co/EDrfN76ToB #england',\n",
              " '#SundaySunsets at the Buffalo Bill Cultural Center, Oakley, Kan. #noplacelikeks #roxieontheroad @TravelKS #mwtravel @_sundaysunsets_ @RoarLoudTravel @always5star @angelsnmom @GreenMochila #traveltribe #travelblogger #travelsomeday https://t.co/mC14nwBIX8',\n",
              " 'Free Accommodation in Exchange for Work as a Hostel Volunteer #travelsomeday #armchairtravel #plannowtravellater https://t.co/bw6oCOHEAF #workaway',\n",
              " 'Virtually travel and explore New Zealand in this article from guest writer &amp; photographer Courtney Ley  &gt;&gt; https://t.co/3aWAZRL7f1  #stayhome #travelsomeday #DreamNowTravelLater https://t.co/yxMzObd462',\n",
              " 'A month before lockdown, I drove around NE #scotland with my little sister. For some aspirational/domestic travel inspo, I got to write about it for @ThePointsGuyUK https://t.co/oMsYlQOCJD #roadtrip #travelsomeday @VisitScotland']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "ckzuwKkfWdDu",
        "outputId": "63fae38e-161e-490f-fd8b-f0259ff02b61"
      },
      "source": [
        "#We have two columns as a text. Let's check them and decide which one we will work on\n",
        "df.loc[:,['quote', 'tweet']].sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quote</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>RT @RoarLoudTravel: Virtually travel and explo...</td>\n",
              "      <td>Virtually travel and explore New Zealand in th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1672</th>\n",
              "      <td>RT @Nancy_Chuang: The rains bring the most unb...</td>\n",
              "      <td>The rains bring the most unbearable 85% humdit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5382</th>\n",
              "      <td>Solo Dining: How to Rock Eating Alone in a Res...</td>\n",
              "      <td>Solo Dining: How to Rock Eating Alone in a Res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5603</th>\n",
              "      <td>RT @clairesturz: Free Accommodation in Exchang...</td>\n",
              "      <td>Free Accommodation in Exchange for Work as a H...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2938</th>\n",
              "      <td>Inis MÃ³r Island: A Day Trip to the Aran Island...</td>\n",
              "      <td>Inis MÃ³r Island: A Day Trip to the Aran Island...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3759</th>\n",
              "      <td>How to Fall in Love With #London in 2 Days #tr...</td>\n",
              "      <td>How to Fall in Love With #London in 2 Days #tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2513</th>\n",
              "      <td>RT @clairesturz: A Giantâ€™s Causeway Tour from ...</td>\n",
              "      <td>A Giantâ€™s Causeway Tour from Belfast to Walk t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1680</th>\n",
              "      <td>Just because you can\\'t travel right now doesn...</td>\n",
              "      <td>Just because you can\\'t travel right now doesn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2890</th>\n",
              "      <td>How many have you seen in person? https://t.co...</td>\n",
              "      <td>How many have you seen in person? https://t.co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2034</th>\n",
              "      <td>El Santuario de las Lajas â€“ #Ipiales #travelso...</td>\n",
              "      <td>El Santuario de las Lajas â€“ #Ipiales #travelso...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  quote                                              tweet\n",
              "391   RT @RoarLoudTravel: Virtually travel and explo...  Virtually travel and explore New Zealand in th...\n",
              "1672  RT @Nancy_Chuang: The rains bring the most unb...  The rains bring the most unbearable 85% humdit...\n",
              "5382  Solo Dining: How to Rock Eating Alone in a Res...  Solo Dining: How to Rock Eating Alone in a Res...\n",
              "5603  RT @clairesturz: Free Accommodation in Exchang...  Free Accommodation in Exchange for Work as a H...\n",
              "2938  Inis MÃ³r Island: A Day Trip to the Aran Island...  Inis MÃ³r Island: A Day Trip to the Aran Island...\n",
              "3759  How to Fall in Love With #London in 2 Days #tr...  How to Fall in Love With #London in 2 Days #tr...\n",
              "2513  RT @clairesturz: A Giantâ€™s Causeway Tour from ...  A Giantâ€™s Causeway Tour from Belfast to Walk t...\n",
              "1680  Just because you can\\'t travel right now doesn...  Just because you can\\'t travel right now doesn...\n",
              "2890  How many have you seen in person? https://t.co...  How many have you seen in person? https://t.co...\n",
              "2034  El Santuario de las Lajas â€“ #Ipiales #travelso...  El Santuario de las Lajas â€“ #Ipiales #travelso..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHpJV9SYXjQc"
      },
      "source": [
        "It looks like quote column is mostly retweets. I will go with ***tweet*** column from now on, and apply text cleaning on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBnBXqQAZ1m4"
      },
      "source": [
        "# **Text Cleaning**\n",
        "\n",
        "In this step, these text cleaning steps will be applied on tweets:\n",
        "\n",
        "\n",
        "\n",
        "*   Make all text lower case\n",
        "*   Expanding Contractions\n",
        "*   Removal of Stop Words\n",
        "*   Removing URLs\n",
        "*   Remove html HTML tags\n",
        "*   Removing Emojis\n",
        "*   Remove Emoticons\n",
        "*   Stemming & Lemmatization (and the difference)\n",
        "*   Spell Correction\n",
        "*   Normalization\n",
        "*   Removing Punctuation\n",
        "*   TF-IDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqTAu8uf_dKy",
        "outputId": "26ef6eec-2889-447e-9582-93d63db14402"
      },
      "source": [
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Km_eY9XauUd"
      },
      "source": [
        "### **Make all text lower case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhqQdkrbXisS",
        "outputId": "dc61fcbc-b6be-4493-80c7-e58e434e79cf"
      },
      "source": [
        "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "df['tweet'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    a tropical treasure to cherish forever ðŸŒ… for e...\n",
              "1    backpacking in #cuba on a budget #travelsomeda...\n",
              "2    we likeee! https://t.co/2g1hmcpsvn #deltaairli...\n",
              "3    experience buenos aires like a local #travelso...\n",
              "4    who can\\'t wait for grenada sailing festival 2...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJwRRxSEtXx_"
      },
      "source": [
        "Before going further, let's extract number of punctuation for each tweet as a new feature\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "jx9PvlxGtkot",
        "outputId": "f42b72a9-0168-4325-fad2-76cd5c7131a6"
      },
      "source": [
        "def count_punct(text):\n",
        "    count = sum([1 for char in text if char in string.punctuation])\n",
        "    return count\n",
        "#Apply the defined function on the text data\n",
        "df['punctuation'] = df['tweet'].apply(lambda x: count_punct(x))\n",
        "#Let's check the dataset\n",
        "df[['tweet','punctuation']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>punctuation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a tropical treasure to cherish forever ðŸŒ… for e...</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>backpacking in #cuba on a budget #travelsomeda...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>we likeee! https://t.co/2g1hmcpsvn #deltaairli...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>experience buenos aires like a local #travelso...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>who can\\'t wait for grenada sailing festival 2...</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  punctuation\n",
              "0  a tropical treasure to cherish forever ðŸŒ… for e...           21\n",
              "1  backpacking in #cuba on a budget #travelsomeda...            9\n",
              "2  we likeee! https://t.co/2g1hmcpsvn #deltaairli...            9\n",
              "3  experience buenos aires like a local #travelso...            9\n",
              "4  who can\\'t wait for grenada sailing festival 2...           25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Y6YEBFHouc8n",
        "outputId": "eccd3964-d9c1-42a6-e359-c80972810b97"
      },
      "source": [
        "# Additionally, let's save number of hastags are new features\n",
        "df['hastags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
        "df[['tweet','hastags']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>hastags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a tropical treasure to cherish forever ðŸŒ… for e...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>backpacking in #cuba on a budget #travelsomeda...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>we likeee! https://t.co/2g1hmcpsvn #deltaairli...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>experience buenos aires like a local #travelso...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>who can\\'t wait for grenada sailing festival 2...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  hastags\n",
              "0  a tropical treasure to cherish forever ðŸŒ… for e...       11\n",
              "1  backpacking in #cuba on a budget #travelsomeda...        4\n",
              "2  we likeee! https://t.co/2g1hmcpsvn #deltaairli...        3\n",
              "3  experience buenos aires like a local #travelso...        4\n",
              "4  who can\\'t wait for grenada sailing festival 2...       11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrmuA_tPGeN5"
      },
      "source": [
        "### **Expand Contractions**\n",
        "\n",
        "Contractions are words or combinations of words which are shortened by dropping letters and replacing them by an apostrophe. \n",
        "\n",
        "Letâ€™s have a look at some examples:\n",
        "* weâ€™re = we are\n",
        "* weâ€™ve = we have\n",
        "* Iâ€™d = I would\n",
        "\n",
        "Note: This step needs to be done before word tokenizer because NLTK word tokenizer has in-built methods for dealing with contractions. However, NLTK word tokenizer's approach separates contractions without expanding. Expanding is a better method than simply separating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXUxHXygNwCE",
        "outputId": "d121eeb4-04bc-430c-b267-114230b0c9d7"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/92/b3c70b8cf2b76f7e3e8b7243d6f06f7cb3bab6ada237b1bce57604c5c519/pyahocorasick-1.4.1.tar.gz (321kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 327kB 4.2MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 266kB 7.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.1-cp37-cp37m-linux_x86_64.whl size=85263 sha256=97c69fd79ce74cb6d971fe4e41b16cd0ec03b15aedca098937c4c01b20284255\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/ab/f7/cb39270df8f6126f3dd4c33d302357167086db460968cfc80c\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.1 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc0NipDDMWtc",
        "outputId": "906df88a-8739-41b0-d154-c3a49f8ea91c"
      },
      "source": [
        "import contractions\n",
        "\n",
        "# Example text \n",
        "text = ''' She'll be airport in 30 mins. We are supposed to catch the arrival, aren't we?  \n",
        "          I'd love to welcome her personally. It'll be an awesome vacation.'''\n",
        "  \n",
        "# creating an empty list \n",
        "expanded_words = []     \n",
        "for word in text.split(): \n",
        "  # using contractions.fix to expand the shotened words \n",
        "  expanded_words.append(contractions.fix(word))    \n",
        "    \n",
        "expanded_text = ' '.join(expanded_words) \n",
        "print('Original text: ' + text) \n",
        "print('\\n') \n",
        "print('Expanded_text: ' + expanded_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text:  She'll be airport in 30 mins. We are supposed to catch the arrival, aren't we?  \n",
            "          I'd love to welcome her personally. It'll be an awesome vacation.\n",
            "\n",
            "\n",
            "Expanded_text: she will be airport in 30 mins. We are supposed to catch the arrival, are not we? I would love to welcome her personally. it will be an awesome vacation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqrXtrIRSRB6",
        "outputId": "d5c1d1a5-ff65-4531-efb3-342aa3efe216"
      },
      "source": [
        "df['tweet'] = df['tweet'].apply(lambda x: contractions.fix(x))\n",
        "df.tweet.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1207    the hotel landscape has changed dramatically. ...\n",
              "1604    a yearly tradition that surprisingly is in pla...\n",
              "198     #travellers, can you guess which country i\\ wo...\n",
              "1605    want to escape to a castle? sintra in portugal...\n",
              "2030    one of the little #hindu shrines dotting the #...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l05k9waN_Ow7"
      },
      "source": [
        "### **Removal of Stop Words** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Zw4duFXWs34o",
        "outputId": "3dbe4fd6-c9a8-413e-b781-70174696c984"
      },
      "source": [
        "# before removing stopwords, they count them\n",
        "df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "df[['tweet','stopwords']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a tropical treasure to cherish forever ðŸŒ… for e...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>backpacking in #cuba on a budget #travelsomeda...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>we likeee! https://t.co/2g1hmcpsvn #deltaairli...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>experience buenos aires like a local #travelso...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>who can\\'t wait for grenada sailing festival 2...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  stopwords\n",
              "0  a tropical treasure to cherish forever ðŸŒ… for e...          4\n",
              "1  backpacking in #cuba on a budget #travelsomeda...          3\n",
              "2  we likeee! https://t.co/2g1hmcpsvn #deltaairli...          1\n",
              "3  experience buenos aires like a local #travelso...          1\n",
              "4  who can\\'t wait for grenada sailing festival 2...          3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrfXgqHc_Syj",
        "outputId": "7f7c92c0-ff86-42bf-f3a2-e0593daab291"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df['tweet'].sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5332    best #hostel #lisbon #portugal solo travellers...\n",
              "2382    blarney castle tour &amp; kissing blarney ston...\n",
              "2574    secret food tour #london #travelsomeday #armch...\n",
              "2587    big changes coming #hawaii tourist experience ...\n",
              "2066    get #cusco #ollantaytambo &amp; go! #travelsom...\n",
              "2365    ireland announces 5-step plan reopening, makes...\n",
              "55      free walking tour #mexico city: estaciÃ³n mexic...\n",
              "3885    improve travel photography can\\'t actually tra...\n",
              "2881    #hiking live #volcano #antigua #guatemala #tra...\n",
              "939     pauline frommer (author frommer guides #nyc) w...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5xmxi4VQS2Y"
      },
      "source": [
        "#### **Adding common words from our document to stop_words**\n",
        "\n",
        "Doing this step can be better after TF-IDF because TF-IDF will also remove many words. Or, at the end of the data cleaning, most frequent words can be checked. If we think that there are some irrelevant or meaningless common words, we can remove them by adding as new stopwords.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhyCQuQYQR8R",
        "outputId": "cd965192-e2fa-4012-ad5f-d5cb529d4223"
      },
      "source": [
        "add_words = [\"also\",\"im\",\"ive\",]\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stop_added = stop_words.union(add_words)\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_added))\n",
        "df['tweet'].sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4565    top things see &amp; #portugal https://t.co/vj...\n",
              "1474    wow, knew #ireland beautiful, idea stunning ge...\n",
              "1421    travel crafts? now\\'s time! @herpackinglist in...\n",
              "1390    anyone tried stand paddleboarding? think looks...\n",
              "3015    try visit #venice day â€“ need one #travelsomeda...\n",
              "3613    best hikes peru â€“ hikes &amp; treks #peru #tra...\n",
              "3255    reflecting previous #travel - you\\'ve visited ...\n",
              "5998    sure start comes getting back traveling planni...\n",
              "4772    stairway gods san miguelito #cancÃºn. ðŸ‡²ðŸ‡½ @museo...\n",
              "3682    good idea keep logbook? â›µï¸ ðŸ“™ info: https://t.c...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kVKfDIHDrmE"
      },
      "source": [
        "### **Remove URLs** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKRT8ArDCBYC",
        "outputId": "121eafab-2703-480e-9c59-2d4104f07d12"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_urls (vTEXT):\n",
        "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "df['tweet'] = df.tweet.apply(remove_urls)\n",
        "df.tweet.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    tropical treasure cherish forever ðŸŒ… exclusive ...\n",
              "1    backpacking #cuba budget #travelsomeday #armch...\n",
              "2    likeee!  #deltaairlines #coronavirus #travelso...\n",
              "3    experience buenos aires like local #travelsome...\n",
              "4    can\\'t wait grenada sailing festival 2021?! â˜ï¸...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q98Z5yJN_tsf"
      },
      "source": [
        "### **Remove html tags** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6w9YSdez_vsD",
        "outputId": "239567ef-e67c-444f-9441-0e2834c1064d"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "\n",
        "#Trial\n",
        "strip_html_tags('sdasdasdasd http://t.co/7AzE4IoGMe Risk Assessmen ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sdasdasdasd http://t.co/7AzE4IoGMe Risk Assessmen '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj8Fup5-ANMB",
        "outputId": "dd4f3689-21db-4f8c-8c25-a137b3f14de0"
      },
      "source": [
        "df['tweet'] = df['tweet'].apply(lambda x: strip_html_tags(x))\n",
        "df['tweet'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    tropical treasure cherish forever ðŸŒ… exclusive ...\n",
              "1    backpacking #cuba budget #travelsomeday #armch...\n",
              "2    likeee!  #deltaairlines #coronavirus #travelso...\n",
              "3    experience buenos aires like local #travelsome...\n",
              "4    can\\'t wait grenada sailing festival 2021?! â˜ï¸...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEnOiQKWOVUf"
      },
      "source": [
        "###**Removing Emojis**\n",
        "\n",
        "Eventhough emojis can be indictor of some emotions that can be related to the tweet, we need to remove the emojis in our text analysis because for now we don't have a strong method to utulize emojis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jTY42KPWO_Zt",
        "outputId": "23e9af67-46c3-4694-f262-55f5bab324df"
      },
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "#Example\n",
        "remove_emoji(\"Have fun with NLP! ðŸ˜ƒðŸ˜ƒ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Have fun with NLP! '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijrm-k45PYcy"
      },
      "source": [
        "# remove all emojis from tweets\n",
        "df['tweet'] = df['tweet'].apply(lambda x: remove_emoji(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO8yws0TPezN"
      },
      "source": [
        "### **Remove Emoticons**\n",
        "\n",
        "In previous steps, we have removed emoji. Now, going to remove emoticons.\n",
        "\n",
        "What is the difference between emoji and emoticons?\n",
        "\n",
        ":-) is an emoticon\n",
        "ðŸ˜œ â†’ emoji."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAVVLr5DPoN5",
        "outputId": "250f42fa-592b-4958-d1f3-d7a560e0a184"
      },
      "source": [
        "!pip install emot\n",
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "\n",
        "# Function for removing emoticons\n",
        "def remove_emoticons(text):\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
        "    return emoticon_pattern.sub(r'', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emot\n",
            "  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n",
            "Installing collected packages: emot\n",
            "Successfully installed emot-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZNz5lKIPxvD",
        "outputId": "80c8d12b-e23a-4f53-ff7e-3cae164016cf"
      },
      "source": [
        "df['tweet'] = df['tweet'].apply(lambda x: remove_emoticons(x))\n",
        "df.tweet.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "202     love hostels, time #adoptahostel! give struggl...\n",
              "698     #pillowchallenge send picture j-pillow travel ...\n",
              "1870    aloha means goodbye: #hawaii extends shutdown ...\n",
              "4679    new blog post! today i\\'m talking home means m...\n",
              "2812    long weekend #riga #latvia #travelsomeday #arm...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6yL83gfScGY"
      },
      "source": [
        "### **Stemming and Lemmatization**\n",
        "\n",
        "**Definition**: Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . \n",
        "\n",
        "Source: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDQ-z4GZS73N"
      },
      "source": [
        "#Example (adapted from datacamp)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "porter=PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFsgLNxNyuJx",
        "outputId": "df9ed9dc-082a-4707-99b3-80acefe4a012"
      },
      "source": [
        "#Tourism specific words to be stemmed and lemmatized\n",
        "word_list = [\"tourist\", \"booking\", \"rating\",\"itinerary\",\"recreation\",\"amenities\",\"attractions\",\"sightseeing\",\"eating\"]\n",
        "print(\"{0:20}{1:20}{2:20}\".format(\"Original Word\",\"Stemmed\",\"Lemmatized\"))\n",
        "\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),wordnet_lemmatizer.lemmatize(word,pos=\"v\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Word       Stemmed             Lemmatized          \n",
            "tourist             tourist             tourist             \n",
            "booking             book                book                \n",
            "rating              rate                rat                 \n",
            "itinerary           itinerari           itinerary           \n",
            "recreation          recreat             recreation          \n",
            "amenities           amen                amenities           \n",
            "attractions         attract             attractions         \n",
            "sightseeing         sightse             sightsee            \n",
            "eating              eat                 eat                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ZtGtbTOySd99",
        "outputId": "fe00dd7c-268d-457c-dd94-0053aca85c74"
      },
      "source": [
        "#We can apply just one of the stemming or lemmatizion. For this study, we will go with lemmatization.\n",
        "#Still here is the codes for stemming\n",
        "'''\n",
        "porter=PorterStemmer()\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda x: stemSentence(x))\n",
        "df.sample(5)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nporter=PorterStemmer()\\n\\ndef stemSentence(sentence):\\n    token_words=word_tokenize(sentence)\\n    token_words\\n    stem_sentence=[]\\n    for word in token_words:\\n        stem_sentence.append(porter.stem(word))\\n        stem_sentence.append(\" \")\\n    return \"\".join(stem_sentence)\\n\\ndf[\\'tweet\\'] = df[\\'tweet\\'].apply(lambda x: stemSentence(x))\\ndf.sample(5)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igk5DrHhVyeP",
        "outputId": "892fd910-1c51-45ce-8661-f9e9827b11ba"
      },
      "source": [
        "#Lemmatization\n",
        "\n",
        "def LemmaSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    lemma_sentence=[]\n",
        "    for word in token_words:\n",
        "        lemma_sentence.append(wordnet_lemmatizer.lemmatize(word, pos= 'v'))\n",
        "        lemma_sentence.append(\" \")\n",
        "    return \"\".join(lemma_sentence)\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda x: LemmaSentence(x))\n",
        "df.tweet.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4611    ten travel tip # cuba # travelsomeday # armcha...\n",
              "2544    plan trip hawaii - momma go travel # travel # ...\n",
              "4695    10 planet\\ 's unusual house # travelsomeday # ...\n",
              "5528    2 days # edinburgh itinerary : perfect weekend...\n",
              "1345    quebecois twilight micmac mountains . @ touris...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkeaoUEMhFx1",
        "outputId": "632e9333-94a6-4f0e-eb8c-8789ec203a85"
      },
      "source": [
        "df.tweet.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('O')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBExBJBbFLri"
      },
      "source": [
        "### **Spell Correction** \n",
        "*Reminder: below explanation was taken from somewhere and needs to be parapharased*\n",
        "\n",
        "\n",
        "Weâ€™ve all seen tweets with a plethora of spelling mistakes. Our timelines are often filled with hastly sent tweets that are barely legible at times.\n",
        "\n",
        "In that regard, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, â€œAnalyticsâ€ and â€œanalytcsâ€ will be treated as different words even if they are used in the same sense.\n",
        "\n",
        "To achieve this we will use the textblob library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNnDCA2qFKyW",
        "outputId": "82954052-0e22-45bc-e875-88672610acb6"
      },
      "source": [
        "from textblob import TextBlob\n",
        "#Example sentence\n",
        "sentence = TextBlob(\"I havv goood speling!\")\n",
        "print(sentence.correct())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I have good spelling!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeDHVwaO1Li4",
        "outputId": "de3fc29f-01b6-4a8d-ddc7-8283401faf18"
      },
      "source": [
        "# Example sentence related to tourism context\n",
        "print(TextBlob('In Chicago, a lakeside is a goodf lcation to relax').correct())\n",
        "print(TextBlob('Minesota is briming with natual and cltural bauty').correct()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In Chicago, a lakeside is a good location to relax\n",
            "Minnesota is brimming with natural and cultural beauty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOMme5UmwDpl",
        "outputId": "6043538e-1f2e-4350-e59c-83398f8af947"
      },
      "source": [
        "%time df['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21.2 s, sys: 104 ms, total: 21.3 s\n",
            "Wall time: 21.4 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    tropical treasure cherish forever exclusive # ...\n",
              "1    backpack # cuba budget # travelsomeday # armch...\n",
              "2    like ! # deltaairlines # coronavirus # travels...\n",
              "3    experience burns air like local # travelsomeda...\n",
              "4    can\\'t wait grenade sail festival 2021 ? ! exc...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a68plr_xhvX9",
        "outputId": "097886cd-162a-4e32-d519-11c9955265a3"
      },
      "source": [
        "df.tweet.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    tropical treasure cherish forever exclusive # ...\n",
              "1    backpack # cuba budget # travelsomeday # armch...\n",
              "2    likeee ! # deltaairlines # coronavirus # trave...\n",
              "3    experience buenos air like local # travelsomed...\n",
              "4    can\\'t wait grenada sail festival 2021 ? ! exc...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "baTeQMhcF0cI"
      },
      "source": [
        "#This takes a bit long to complete. I will skip for now.\n",
        "\n",
        "from textblob import TextBlob\n",
        "%time df['tweet'] = df['tweet'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "print(df.tweet.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fr4O3SZiGg6E"
      },
      "source": [
        "# Checking misspelled words (we aren't using this in this study)\n",
        "\n",
        "# !pip install pyspellchecker\n",
        "# from spellchecker import SpellChecker\n",
        "\n",
        "# spell = SpellChecker()\n",
        "# misspelled = spell.unknown(df.tweet)\n",
        "# print(misspelled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX4z0Fb--10J"
      },
      "source": [
        "### **Removing Punctuation**\n",
        "\n",
        "A reminder: if you remove punctuation earlier then html tags and URLs, then it won't be possible to remove them probably later. That's why, removing punctuation the last is a wise choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrTpn_NQUgX8",
        "outputId": "2d42b21b-f92f-4561-b097-2773656e4b81"
      },
      "source": [
        "df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n",
        "df['tweet'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    tropical treasure cherish forever exclusive  t...\n",
              "1    backpack  cuba budget  travelsomeday  armchair...\n",
              "2    like   deltaairlines  coronavirus  travelsomeday \n",
              "3    experience burns air like local  travelsomeday...\n",
              "4    cant wait grenade sail festival 2021   exclusi...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dWVYQKM6K-i"
      },
      "source": [
        "### **Language Detection**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bPosLDK-6KOp",
        "outputId": "f95f6236-f3d2-4de7-dc1b-26dbc98892bc"
      },
      "source": [
        "import os, re, nltk, spacy, string, umap\n",
        "\n",
        "pd.set_option('display.max_colwidth', 50) \n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "!pip install spacy-langdetect\n",
        "from spacy_langdetect import LanguageDetector\n",
        "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-langdetect\n",
            "  Downloading https://files.pythonhosted.org/packages/29/70/72dad19abe81ca8e85ff951da170915211d42d705a001d7e353af349a704/spacy_langdetect-0.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from spacy-langdetect) (3.6.4)\n",
            "Collecting langdetect==1.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy-langdetect) (8.7.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy-langdetect) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->spacy-langdetect) (54.0.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy-langdetect) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy-langdetect) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy-langdetect) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spacy-langdetect) (20.3.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp37-none-any.whl size=993460 sha256=35d43eef096d5d626f1ccea9d353094ef4ef9244020020e19332d13e2377dc78\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect, spacy-langdetect\n",
            "Successfully installed langdetect-1.0.7 spacy-langdetect-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OwqyU_sX6zsj",
        "outputId": "15f099aa-7c11-49d5-a3d1-667c785c02b8"
      },
      "source": [
        "def detect_language(tweet):\n",
        "    try:\n",
        "        doc = nlp(t)\n",
        "        language = doc._.language['language']\n",
        "        score = doc._.language['score']\n",
        "    except:\n",
        "        language = ''\n",
        "        print('sth goes wrong')\n",
        "    return language, score\n",
        "\n",
        "languages = []\n",
        "scores = []\n",
        "for i, t in df['tweet'].iteritems():\n",
        "    l, s = detect_language(t)\n",
        "    if i % 100 ==0:\n",
        "        print(i, t, l)  \n",
        "    languages.append(l)\n",
        "    scores.append(s)\n",
        "    \n",
        "df['lng'] = languages\n",
        "df['lng_score'] = scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tropical treasure cherish forever exclusive  traveltips island insight visit  grenade  timetolime  puregrenada  spiceisle  ilovegrenada  dreamtodaytraveltomorrow  travelsomeday  webelieveintravel  grenadadreaming  caribbeandreaming  en\n",
            "100 charlie chocolate factory afternoon tea  london  travelsomeday  armchairtravel  staythefhome  england  en\n",
            "200 nuestros compaÃ±eros de travel est abriendo frontiers captando naevus ideas para el tourism de nuestra provincial     stop chaos son nos verdaderos cracks   tourism  evil  travelsomeday  es\n",
            "300  travelsomeday  no\n",
            "400 virtually travel explore new zealand article guest writer  photographer journey let    stayhome  travelsomeday  dreamnowtravellater  en\n",
            "500 sometimes sasha draconine  othertimes look like this   thailandinsider  thailandfanclub  gothaibefree  thailand  bangkok  watprakaew  we  tt  trot  travel  travelblogger  solotravel  gaytravel  ilovelgbttravel  travelfromhome  travelsomeday  armchairtravel  picoftheday  en\n",
            "600 wonder  hokusai know influential  the great  wave  kanagawa  would be  nearly 200 years later   streetart  graffitiart  graffiti  art  visit  visit  ilovelgbttravel  gaytravel  travel  travelfromhome  virtualtravel  traveltomorrow  travelsomeday  we  trot  tt  en\n",
            "700 may thing anaemic era hope stick around  deltaairlines  travelsomeday  en\n",
            "800 great mix architecture  nighttime best thermal bath world   budapest one favorite cities  europe intrigue  check last trip you figure  travelsomeday  en\n",
            "900 walk artists  alley  kansascity   visit  ilovelgbttravel  streetart  graffiti  art  gaytravel  lesbiantravel  we  tt  trot  travel  travelfromhome  armchairtravel  travelsomeday  traveltomorrow  virtualtravel  are  lgbttravel  travelblogger  solotravel  are  no\n",
            "1000 happy  earthly everyone  favorite photo celebrate natural wonder earth  share favorite photo celebrate great planet   earthday2020  wednesdaymorning  stayhome  travelsomeday  wednesdaymotivation  en\n",
            "1100 day trip  right  latvia  travelsomeday  armchairtravel  staythefhome  en\n",
            "1200 beach here s printable beachtheme coloringin sheet activity sheet kid   dreamnowtravellater  sunset  bucketlist  travelsomeday  en\n",
            "1300  love affair  barcelona  travelsomeday  armchairtravel  staythefhome  adventure  en\n",
            "1400 anyone try stand paddleboarding  think look like much fun  especially beautiful  sunrise backdrop   dreamnowtravellater  travelsomeday  bucketlist  en\n",
            "1500  coronavirus change hope stick around  travelsomeday  deltaairlines  en\n",
            "1600 virtual travel pleasure  give sun view dry creek valley   winecountry  travelsomeday pp   omarshinez  view terrace  trattorefarms   en\n",
            "1700 americans find shut try travel near future   travelsomeday  coronavirus  en\n",
            "1800 head  india it s safe travel again  check  photography tip make tax mahan photo  travelphotography  travel  traveltips  travelblog  travelblogger  femaletravelblogger  traveltribe  agra  travelsomeday  en\n",
            "1900 loops hashtags work add fact   srilanka  travelphotography  travelsomeday  poyaday  srilankadaily  la  colombia  wanderlust  en\n",
            "2000 date woman travel  travelsomeday  armchairtravel  staythefhome  traveltips  en\n",
            "2100 2019 seem like lifetime ago  doesnt it  simpler time  reveal popular adventure wanderlust trip finder last year  classic surprise list   armchairwanderlust  travelsomeday  en\n",
            "2200  galswander  fitlifetravel  liveamemory  eatlivestay  angelsnmom  always5star  _sundaysunsets_  roarloudtravel  greenmochila  evanskerry  emilymaehood  touches  travelsendless  apairofleclairs  aramblinwoman  ourworldforyou  waysyouwander  yourcruisegurl  lisarivera2207  travelbugsworld  jetsettersflyin  perthtravelers  suziday123  travelwithirine  usgulfcoast  fullvanfun  giselleinmotion  folderred  onedelightfull  xeniakaepernick  stevehammactor  stevekubota  abfabtravels  roadtripc  jflorez  demicassiani  coursecharted  susanbryenton  hhlifestyletrav  organicroadmap  monkeysventures  nicolette_o  guesswhereiwent  adventuringgal  curious miss big adventure watch sunset offthebeaten track place   sundaysunsets take fall star valley mesh island  ran  _sundaysunsets_  always5star  roarloudtravel  greenmochila  angelsnmom  sundaymotivation  travelsomeday  en\n",
            "2300 latest update  ray  sail course charter  grenade   grenadiers  puregrenada  ilovegrenada  dreamtodaytraveltomorrow  travelsomeday  webelieveintravel  grenadadreaming  caribbeandreaming  en\n",
            "2400 11 reason  hostess awesome  travelsomeday  armchairtravel  staythefhome  travel  en\n",
            "2500 one best ways see labor above   dreamnowvisitlater  labor  egypt  adventuretravel  travel  history  divergenttravel  travelsomeday  experienceegypt  en\n",
            "2600 grab adventure partner discover egypt hold  photo location  karay complex labor egypt  dreamnowvisitlater  labor  egypt  adventuretravel  travel  history  divergenttravel  travelsomeday  experienceegypt  travelcouple  en\n",
            "2700 get nature could good strategy travel again  inspiration   travelsomeday  en\n",
            "2800 archivesso book longer new  picture still aweinspiring   travelsomeday  adventuresomeday  en\n",
            "2900 go watch  privates  bone  you want plan future trip one 9 private hot spot around world   travelsomeday  armchairwanderlust  en\n",
            "3000  travelsomeday fun things western massachusetts  othersidema  othersidema  en\n",
            "3100 would pay  200 avoid 2week quarante upon arrival  europe   travelsomeday  austria  en\n",
            "3200 today noon  est   scott_history  live washington s war tent join  mountvernon s  douglasbradburn  live washington s estate   mountvernon s  washingtonwednesday  tune   amrevmuseum via  uwishunu  travelsomeday  en\n",
            "3300  hobbitenango   hobbies guatemala  s middle earth  travelsomeday  armchairtravel  staythefhome  antique  en\n",
            "3400 maman scrawl capital jordan  hide beneath bustle you find wealth history culture  add  visitjordan  travelsomeday bucket list   dreamnowtravellater  maman  divergenttravel  jordan  adventuretravel  en\n",
            "3500 need know you plan visit national park come months  travelsomeday  en\n",
            "3600 normally would spend long weekend something like   spoke parksville  road less travel  travelsomeday  explored  en\n",
            "3700  travelsomeday best hot air balloon experience around world  en\n",
            "3800  travelsomeday play natalya golf  two great golf course old course new course heart  costadelsol  golf play natalya golf  two great golf course old course  new course heart  costadelsol  golf  expand  spain  egtgolftour  staysafe  en\n",
            "3900 take armchair trip around south africa s cape peninsula me   bloggerls  lovingblogs  travelsomeday  en\n",
            "4000 past make cook class  milan  italy  armchairtravel  travelsomeday  staythefhome  fooddrink  en\n",
            "4100 portugal s odd open plan may make neighbor irate  play american brazilian travelers  travelsomeday  coronavirus  en\n",
            "4200 11 unmissable experience activities  paris kid  lovingblogs  bloggerls  travelsomeday  en\n",
            "4300 solo din  rock eat alone restaurant  armchairtravel  travelsomeday  staythefhome  fooddrink  en\n",
            "4400 free things vancouver   canada  travelsomeday  armchairtravel  plannowtravellater  en\n",
            "4500  portugal reward countries speak  portuguese  large community descend portugal  open measure  travelsomeday  en\n",
            "4600 bite  berlin food tour review  travelsomeday  armchairtravel  plannowtravellater  europe  en\n",
            "4700 it easier find room national park lodge summer find space national park campfire  here s why   travelsomeday  en\n",
            "4800 best budget destination add bucket list  travelsomeday  armchairtravel  plannowtravellater  en\n",
            "4900 free things  toronto  canada  travelsomeday  armchairtravel  plannowtravellater  en\n",
            "5000 find campfire re could unusually difficult year  thank increase number re vacationers  close  or partially close campgrounds  we help easy fourstep strategy   travelsomeday  en\n",
            "5100 estates preparando el major  vice for  italian serf un experience personalizada incredible  quires veins   travelsomeday  travelers  travelphotography  nuevanormalidad  traveler  es\n",
            "5200 things  every  portugal  travelsomeday  armchairtravel  plannowtravellater  europe  en\n",
            "5300 new post fell love seville  spain  ever travel somewhere instantly fall love place  maybe even plan move there   travelblog  traveltribe  seville  spain  travelsomeday  travelmemories  en\n",
            "5400 harvey bay  town know two reason  popular access point fraser island and  season  great stop whale watch  queensland  australia  travel  travelsomeday  travelphotography  longer  blow  en\n",
            "5500 colourful cities world  travelsomeday  armchairtravel  plannowtravellater  colour  en\n",
            "5600 castle intra  fairytale palaces  garden historical sit  travelsomeday  armchairtravel  plannowtravellater  lesion  en\n",
            "5700 your valley wine tour  river cruise  porto  travelsomeday  armchairtravel  plannowtravellater  portwine  en\n",
            "5800 manage attitude  custom attitude sickness much pitch  travelsomeday  armchairtravel  plannowtravellater  peru  en\n",
            "5900 best view pen palace  garden intra   portugal  travelsomeday  armchairtravel  plannowtravellater  europe  en\n",
            "6000 7 best free view  london  travelsomeday  armchairtravel  plannowtravellater  en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_rCL69hJ7tn3",
        "outputId": "fc5350f2-ee52-4743-8e2c-e3ed0e66c424"
      },
      "source": [
        "df[['tweet', 'lng','lng_score']].sample(10).round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>lng</th>\n",
              "      <th>lng_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>lavender field  an river  moles writes outdoor...</td>\n",
              "      <td>en</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4359</th>\n",
              "      <td>djurÃ¶nÃ¤set  relax  stockholm archipelago  tra...</td>\n",
              "      <td>sv</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1162</th>\n",
              "      <td>friends beat page travel include wichita  s ke...</td>\n",
              "      <td>en</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2521</th>\n",
              "      <td>2 days  edinburgh itinerant  perfect weekend  ...</td>\n",
              "      <td>en</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4807</th>\n",
              "      <td>manage attitude  custom attitude sickness much...</td>\n",
              "      <td>en</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5134</th>\n",
              "      <td>xavc67 many case you right that s include has...</td>\n",
              "      <td>en</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5036</th>\n",
              "      <td>eat venice  traditional food  venice try  trav...</td>\n",
              "      <td>en</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2175</th>\n",
              "      <td>visit frederiksborg castle hillerÃ¸d  denmark  ...</td>\n",
              "      <td>da</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5264</th>\n",
              "      <td>like simon mountains  ethiopia need add anyone...</td>\n",
              "      <td>no</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2255</th>\n",
              "      <td>climb  of arena nine partner of  travelsomeday...</td>\n",
              "      <td>en</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet lng  lng_score\n",
              "2497  lavender field  an river  moles writes outdoor...  en       1.00\n",
              "4359   djurÃ¶nÃ¤set  relax  stockholm archipelago  tra...  sv       0.86\n",
              "1162  friends beat page travel include wichita  s ke...  en       1.00\n",
              "2521  2 days  edinburgh itinerant  perfect weekend  ...  en       1.00\n",
              "4807  manage attitude  custom attitude sickness much...  en       1.00\n",
              "5134   xavc67 many case you right that s include has...  en       1.00\n",
              "5036  eat venice  traditional food  venice try  trav...  en       1.00\n",
              "2175  visit frederiksborg castle hillerÃ¸d  denmark  ...  da       1.00\n",
              "5264  like simon mountains  ethiopia need add anyone...  no       0.57\n",
              "2255  climb  of arena nine partner of  travelsomeday...  en       1.00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZJsxcvQG78SR",
        "outputId": "7620a4fd-516f-4307-d017-c977e8742de7"
      },
      "source": [
        "df.lng.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "en    5607\n",
              "no     186\n",
              "it      76\n",
              "fr      38\n",
              "sv      35\n",
              "da      33\n",
              "ca      26\n",
              "es      20\n",
              "id       2\n",
              "nl       1\n",
              "hr       1\n",
              "el       1\n",
              "sl       1\n",
              "Name: lng, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVv90u_59LxM"
      },
      "source": [
        "language_counts = df.lng.value_counts()\n",
        "pd.DataFrame(language_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU-fq2HM8mg8"
      },
      "source": [
        "'no' stands for Norwegian language. \n",
        "\n",
        "For more info about acronyms of languages:\n",
        "https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wWKFsGX6BBYs",
        "outputId": "8d955daa-e675-4e94-b069-2883ee921f90"
      },
      "source": [
        "# Number of English Tweets\n",
        "df.loc[df.lng == 'en'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5607, 43)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjE-_dclA3aw"
      },
      "source": [
        "# Keep only English language and drop the rest\n",
        "df = df.loc[df.lng == 'en']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxWEoW9WsdDE"
      },
      "source": [
        "### **TF_IDF**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqUr8SRLsekn"
      },
      "source": [
        "#import the TfidfVectorizer from Scikit-Learn.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=.80, min_df=5, stop_words=None, use_idf=True, norm=None)\n",
        "transformed_tweets = vectorizer.fit_transform(df.tweet)\n",
        "\n",
        "print(transformed_tweets.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1PqzEfKvSzJ"
      },
      "source": [
        "dense = transformed_tweets.todense()\n",
        "denselist = dense.tolist()\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "df2 = pd.DataFrame(denselist, columns=feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDTbEAtJyTJN"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Y0ruNDyUrB"
      },
      "source": [
        "transformed_tweets_as_array = transformed_tweets.toarray()\n",
        "# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
        "len(transformed_tweets_as_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us5zshUWWe-P"
      },
      "source": [
        "### **Tokenization**\n",
        "\n",
        "\n",
        "For many of the techniques we'll be using in future notebooks, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7QkoYMgWvWz"
      },
      "source": [
        "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "data_cv = cv.fit_transform(df.tweet)\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_dtm.index = df.index\n",
        "data_dtm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMm9bfRMYhU8"
      },
      "source": [
        "cv = CountVectorizer(max_df = 0.90, min_df= 0.001)\n",
        "data_cv = cv.fit_transform(df.tweet)\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names()) #Document term matrix\n",
        "data_dtm.index = df.index\n",
        "data_dtm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPKJ80fzjfN2"
      },
      "source": [
        "# Let's pickle it for later use\n",
        "data_dtm.to_pickle(\"cleaned_tweets.pkl\")\n",
        "\n",
        "#Notice that the pickled data contains only tweets, not other columns.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwZynJP2lAZ_"
      },
      "source": [
        "\n",
        "**What Can You Do With pickle?**\n",
        "\n",
        "Pickling is useful for applications where you need some degree of persistency in your data. \n",
        "Your program's state data can be saved to disk, so you can continue working on it later on.\n",
        "It can also be used to send data over a Transmission Control Protocol (TCP) or \n",
        "socket connection, or to store python objects in a database. Pickle is very \n",
        "useful for when you're working with machine learning algorithms, where you want\n",
        "to save them to be able to make new predictions at a later time, without\n",
        "having to rewrite everything or train the model all over again.\n",
        "\n",
        "Source:https://www.datacamp.com/community/tutorials/pickle-python-tutorial#whatfor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1so2ttI-RFDT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# **FEATURE EXTRACTION**\n",
        "\n",
        "For the feature extraction, we will apply following steps in order. Depending on your study and data, you may skip some of these features and add your own feature extractions.\n",
        "\n",
        " word count: counts the number of tokens in the text (separated by a space)\n",
        "\n",
        "1.   **character count**: \n",
        "2. **average word length**\n",
        "\n",
        "\n",
        "Features that can only be obtained before the text cleaning\n",
        "\n",
        "\n",
        "* Number of stop words\n",
        "* The number of punctuation\n",
        "* Number of hashtag characters\n",
        "* Number of numerical characters\n",
        "* Number of Uppercase words\n",
        "* sentence count\n",
        "\n",
        "In the book, as an egzersize, it can be asked: Why these features can only be obtained before text cleaning? Explain for each of them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9nTDz3mCs11"
      },
      "source": [
        "'''\n",
        "# Here is the feature enginnering scripts that can be done before data cleaning\n",
        "\n",
        "# Number of stopwords\n",
        "df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "\n",
        "#Number of punctuation\n",
        "def count_punct(text):\n",
        "    count = sum([1 for char in text if char in string.punctuation])\n",
        "    return count\n",
        "\n",
        "df['punctuation'] = df['tweet'].apply(lambda x: count_punct(x))\n",
        "\n",
        "# Number of hashtag characters\n",
        "df['hastags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
        "\n",
        "# Number of numerics\n",
        "df['numerics'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
        "\n",
        "# Number of Uppercase words\n",
        "df['upper'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "\n",
        "# sentence count\n",
        "dtf['sentence_count'] = df[\"tweet\"].apply(lambda x: len(str(x).split(\".\")))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-40Rj6584iYG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6uL7_6pgpkA"
      },
      "source": [
        "# Number of hashtag characters\n",
        "df['hastags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
        "\n",
        "#character count\n",
        "df['char_count'] = df['tweet'].str.len() ## this also includes spaces\n",
        "\n",
        "def avg_word(sentence):\n",
        "  words = sentence.split()\n",
        "  return (sum(len(word) for word in words)/(len(words)+0.000001))\n",
        "\n",
        "df['avg_word'] = df['tweet'].apply(lambda x: avg_word(x)).round(1)\n",
        "\n",
        "df[['tweet','char_count','avg_word']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSfh4Ux-jHU3"
      },
      "source": [
        "Keep in mind that in feature enginnering process, we are not changing the text (the tweet) column. Therefore, the pickled file stays unchanged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHZucMsyBHGV"
      },
      "source": [
        "#Let's save cleaned data\n",
        "\n",
        "df.to_csv('tourism_tweets_cleaned.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XXaKjRsgjpj"
      },
      "source": [
        "### **Checking requirements for this notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYFillrDf-hf"
      },
      "source": [
        "!pip3 freeze > requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Efk1LLfgd3m"
      },
      "source": [
        "# show the file's contents\n",
        "! cat requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OhOJF3uDpoM",
        "outputId": "9f0807fd-76a6-4c23-a0df-b52914593d83"
      },
      "source": [
        "# Import the os module\n",
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Current working directory: {0}\".format(cwd))\n",
        "\n",
        "# Print the type of the returned object\n",
        "print(\"os.getcwd() returns an object of type: {0}\".format(type(cwd)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "os.getcwd() returns an object of type: <class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}